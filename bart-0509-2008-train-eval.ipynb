{"cells":[{"cell_type":"markdown","metadata":{"id":"AvCot6IFw3LG"},"source":["# Abstractive summaries - Train DistilBART on TWEETSUMM dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:06:56.910806Z","iopub.status.busy":"2024-09-04T21:06:56.910479Z","iopub.status.idle":"2024-09-04T21:07:17.567180Z","shell.execute_reply":"2024-09-04T21:07:17.566388Z","shell.execute_reply.started":"2024-09-04T21:06:56.910771Z"},"id":"csIsnw2147iB","outputId":"b1168797-2e98-4a6a-81e1-9e079f98ab53","scrolled":true,"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","import pandas as pd\n","import numpy as np\n","import os, time, datetime\n","\n","from datasets import Dataset, DatasetDict\n","\n","from transformers import DataCollatorForSeq2Seq, AutoTokenizer, set_seed\n","from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:17.569198Z","iopub.status.busy":"2024-09-04T21:07:17.568901Z","iopub.status.idle":"2024-09-04T21:07:20.900417Z","shell.execute_reply":"2024-09-04T21:07:20.899214Z","shell.execute_reply.started":"2024-09-04T21:07:17.569165Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!pip freeze > requirements_bart_a100.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:20.902419Z","iopub.status.busy":"2024-09-04T21:07:20.902044Z","iopub.status.idle":"2024-09-04T21:07:21.299315Z","shell.execute_reply":"2024-09-04T21:07:21.298334Z","shell.execute_reply.started":"2024-09-04T21:07:20.902368Z"},"id":"D7StYsAaxJLT","outputId":"3690cf47-554a-424d-b42e-e8f77bb4bbf5","trusted":true},"outputs":[],"source":["ds_dir = os.path.join(os.getcwd(), 'data')\n","try:\n","    HF_TOKEN =  os.environ['HF_TOKEN']\n","except:\n","    HF_TOKEN = \"\"\n","\n","if 'google.colab' in str(get_ipython()):\n","    print(\"Running on Colab\")\n","    from google.colab import drive, userdata\n","    drive.mount('/content/drive')\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","elif os.environ.get('KAGGLE_KERNEL_RUN_TYPE') != None:\n","    ds_dir = '/kaggle/input/bertdata2207/'\n","    # ds_dir=\"/kaggle/input/bertdata2207/\"\n","    from kaggle_secrets import UserSecretsClient\n","    print(\"Running on Kaggle\")\n","    # ds_dir = \"/kaggle/input/tweet-data-2106-1512/\"\n","    user_secrets = UserSecretsClient()\n","    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n","    WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n","    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:21.301930Z","iopub.status.busy":"2024-09-04T21:07:21.301609Z","iopub.status.idle":"2024-09-04T21:07:21.312921Z","shell.execute_reply":"2024-09-04T21:07:21.312036Z","shell.execute_reply.started":"2024-09-04T21:07:21.301898Z"},"id":"XCVrFPA4GL-h","outputId":"228dbbaf-c276-495e-e759-0ecb0b4a1ebb","scrolled":true,"trusted":true},"outputs":[],"source":["set_seed(17)\n","os.environ[\"WANDB_PROJECT\"] = \"aiml-thesis-train-test\""]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdawidk5\u001b[0m (\u001b[33mdawidk5ul\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.8"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/dawidk/bartabs/aiml-thesis/wandb/run-20240905_210401-vsgdh885</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/dawidk5ul/aiml-thesis-train-test/runs/vsgdh885' target=\"_blank\">resilient-plant-1</a></strong> to <a href='https://wandb.ai/dawidk5ul/aiml-thesis-train-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/dawidk5ul/aiml-thesis-train-test' target=\"_blank\">https://wandb.ai/dawidk5ul/aiml-thesis-train-test</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/dawidk5ul/aiml-thesis-train-test/runs/vsgdh885' target=\"_blank\">https://wandb.ai/dawidk5ul/aiml-thesis-train-test/runs/vsgdh885</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dawidk5ul/aiml-thesis-train-test/runs/vsgdh885?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fa231cba870>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(settings=wandb.Settings(start_method=\"thread\"))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QoMm76b2i2NX","outputId":"f5df06b4-ac4d-4be9-8044-20010745149b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/dawidk/.cache/huggingface/token\n","Login successful\n"]}],"source":["login(token=HF_TOKEN)"]},{"cell_type":"markdown","metadata":{"id":"xP2HgEaKi2Nk"},"source":["## Load data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:21.328955Z","iopub.status.busy":"2024-09-04T21:07:21.328650Z","iopub.status.idle":"2024-09-04T21:07:21.337951Z","shell.execute_reply":"2024-09-04T21:07:21.337144Z","shell.execute_reply.started":"2024-09-04T21:07:21.328905Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/dawidk/bartabs/aiml-thesis/data\n"]}],"source":["print(ds_dir)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:21.339201Z","iopub.status.busy":"2024-09-04T21:07:21.338926Z","iopub.status.idle":"2024-09-04T21:07:21.347112Z","shell.execute_reply":"2024-09-04T21:07:21.346166Z","shell.execute_reply.started":"2024-09-04T21:07:21.339171Z"},"trusted":true},"outputs":[],"source":["checkpoint_bart = \"sshleifer/distilbart-xsum-12-6\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:21.348478Z","iopub.status.busy":"2024-09-04T21:07:21.348122Z","iopub.status.idle":"2024-09-04T21:07:21.437962Z","shell.execute_reply":"2024-09-04T21:07:21.437073Z","shell.execute_reply.started":"2024-09-04T21:07:21.348445Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dialogue    string[python]\n","summary     string[python]\n","dtype: object\n","                                            dialogue  \\\n","0  Customer: So neither my iPhone nor my Apple Wa...   \n","1  Customer: @115850 hi team! i m planning to get...   \n","2  Customer: @AskAmex Where do I write to address...   \n","3  Customer: @AmazonHelp @115821 Wow, expected 4 ...   \n","4  Customer: @GWRHelp I'd rather you spent some t...   \n","\n","                                             summary  \n","0  Customer enquired about his Iphone and Apple w...  \n","1  Customer is eager to know about the replacemen...  \n","2  Signed up for an AmexCard with Delta but it di...  \n","3  The customer have a problem. The agent is very...  \n","4  Customer cannot purchase a train ticket on the...  \n"]}],"source":["train_df_temp = pd.read_csv(os.path.join(ds_dir,\"dials_abs_2607_1312_train_spc.csv\"), names=['conv_id','dialogue','summary'], encoding='utf-8', dtype={'conv_id':'string', 'dialogue':'string', 'summary': 'string'})\n","train_df_temp.convert_dtypes()\n","train_df_temp.drop(columns=['conv_id'], inplace=True)\n","train_df_temp.reset_index(drop=True, inplace=True)\n","\n","val_df_temp = pd.read_csv(os.path.join(ds_dir,\"dials_abs_2607_1312_valid_spc.csv\"), names=['conv_id','dialogue','summary'], encoding='utf-8', dtype={'conv_id':'string', 'dialogue':'string', 'summary': 'string'})\n","val_df_temp.convert_dtypes()\n","val_df_temp.drop(columns=['conv_id'], inplace=True)\n","val_df_temp.reset_index(drop=True, inplace=True)\n","\n","test_df_temp = pd.read_csv(os.path.join(ds_dir,\"dials_abs_2607_1312_test_spc.csv\"), names=['conv_id','dialogue','summary'], encoding='utf-8', dtype={'conv_id':'string', 'dialogue':'string', 'summary': 'string'})\n","test_df_temp.convert_dtypes()\n","test_df_temp.reset_index(drop=True, inplace=True)\n","\n","print(train_df_temp.dtypes)\n","print(train_df_temp.head())\n","\n","PD_DATASETS = {'train': train_df_temp, 'validation': val_df_temp, 'test': test_df_temp}"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:07:21.441293Z","iopub.status.busy":"2024-09-04T21:07:21.440982Z","iopub.status.idle":"2024-09-04T21:07:21.496843Z","shell.execute_reply":"2024-09-04T21:07:21.496092Z","shell.execute_reply.started":"2024-09-04T21:07:21.441261Z"},"trusted":true},"outputs":[],"source":["tweetsumm_abs = DatasetDict(\n","    {\n","        'train': Dataset.from_pandas(train_df_temp),\n","        'validation': Dataset.from_pandas(val_df_temp),\n","        'test': Dataset.from_pandas(test_df_temp)\n","    }\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:08:56.321155Z","iopub.status.busy":"2024-09-04T21:08:56.320415Z","iopub.status.idle":"2024-09-04T21:08:57.800812Z","shell.execute_reply":"2024-09-04T21:08:57.799875Z","shell.execute_reply.started":"2024-09-04T21:08:56.321113Z"},"id":"F-hzIHQ9w3Lb","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BartTokenizerFast(name_or_path='sshleifer/distilbart-xsum-12-6', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n","}\n"]},{"name":"stderr","output_type":"stream","text":["/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(checkpoint_bart)\n","print(tokenizer)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:09:07.464785Z","iopub.status.busy":"2024-09-04T21:09:07.464378Z","iopub.status.idle":"2024-09-04T21:09:07.471198Z","shell.execute_reply":"2024-09-04T21:09:07.470182Z","shell.execute_reply.started":"2024-09-04T21:09:07.464748Z"},"id":"vVQdGPfZw3Lb","trusted":true},"outputs":[],"source":["# Source: https://huggingface.co/docs/transformers/en/tasks/summarization\n","\n","def preprocess_function(examples):\n","    prefix = \"summarize: \"\n","    inputs = [str(prefix) + str(dial) for dial in examples[\"dialogue\"]]\n","    with tokenizer.as_target_tokenizer():\n","        model_inputs = tokenizer(inputs, max_length=512, truncation=True) # same params as tweetsumm paper\n","        labels = tokenizer(text_target=examples[\"summary\"], max_length=80, truncation=True)\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    print(model_inputs.keys())\n","    return model_inputs"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:52:07.733531Z","iopub.status.busy":"2024-09-04T21:52:07.733105Z","iopub.status.idle":"2024-09-04T21:52:08.561712Z","shell.execute_reply":"2024-09-04T21:52:08.560780Z","shell.execute_reply.started":"2024-09-04T21:52:07.733492Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"274935ef1c2d4d3ea1034d9728d9b001","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/867 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'attention_mask', 'labels'])\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fff7e0e259b7440f83a08808a9cf71dc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/110 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'attention_mask', 'labels'])\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"227411c725c148c8a013de9ceb98bdea","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/109 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'attention_mask', 'labels'])\n","{'input_ids': [0, 18581, 3916, 2072, 35, 19458, 35, 787, 1225, 4432, 1096, 20280, 165, 328, 939, 475, 1884, 7, 120, 1257, 1754, 510, 20529, 27785, 24, 924, 15, 5, 998, 24, 34, 158, 360, 5010, 21784, 6, 64, 1717, 3922, 162, 99, 16, 24, 17487, 50118, 45443, 35, 787, 2481, 3897, 2036, 166, 348, 10, 158, 7033, 5010, 714, 114, 5, 6880, 47, 829, 16, 5009, 50, 31559, 4, 37249, 10237, 50118, 44799, 35, 787, 25146, 28780, 5148, 27785, 125, 99, 114, 939, 399, 17, 27, 90, 101, 5, 1152, 8, 236, 7, 671, 24, 50118, 45443, 35, 787, 2481, 3897, 2036, 166, 1979, 75, 28, 441, 7, 3264, 5, 23312, 2886, 4, 286, 55, 335, 15, 1830, 2886, 714, 4, 17161, 352, 3753, 15, 5, 3104, 1373, 259, 35, 1205, 640, 90, 4, 876, 73, 571, 40969, 9380, 530, 4154, 510, 975, 4, 3166, 19954, 877, 110, 2969, 4, 50118, 44799, 35, 787, 25146, 28780, 5148, 2446, 27785, 125, 209, 32, 5567, 15797, 98, 473, 24, 1266, 276, 714, 3253, 13, 209, 25, 157, 50118, 45443, 35, 787, 2481, 3897, 2036, 3216, 6, 30845, 73, 5567, 15797, 32, 45, 4973, 13, 23312, 2886, 4, 96, 403, 9, 143, 1880, 73, 17584, 47, 64, 1338, 66, 7, 201, 6, 52, 581, 1649, 8, 244, 47, 14649, 4, 37249, 846, 487, 50118, 44799, 35, 787, 25146, 28780, 5534, 15983, 27785, 4557, 13, 5, 244, 27785, 7840, 10874, 44660, 50118, 45443, 35, 787, 2481, 3897, 2036, 4557, 7, 47, 350, 13, 2969, 4, 1832, 489, 201, 1278, 13, 143, 617, 1379, 4, 166, 581, 28, 1372, 7, 244, 4, 17841, 27969, 37249, 846, 487, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 44799, 16, 7921, 7, 216, 59, 5, 5010, 714, 15, 5, 5567, 15797, 37, 8605, 7, 907, 4, 18497, 2305, 14, 24, 129, 11459, 114, 5, 829, 6880, 16, 31559, 50, 5009, 4, 2]}\n"]}],"source":["tokenized_tweetsumm_abs = tweetsumm_abs.map(preprocess_function, batched=True, remove_columns=['dialogue','summary'])\n","print(tokenized_tweetsumm_abs[\"train\"][1])"]},{"cell_type":"markdown","metadata":{"id":"97ha331Ii2Np"},"source":["## Setup Training Evaluation"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:09:29.766767Z","iopub.status.busy":"2024-09-04T21:09:29.766008Z","iopub.status.idle":"2024-09-04T21:10:46.877759Z","shell.execute_reply":"2024-09-04T21:10:46.876546Z","shell.execute_reply.started":"2024-09-04T21:09:29.766718Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: evaluate in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (0.4.2)\n","Requirement already satisfied: pyrouge in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (0.1.3)\n","Requirement already satisfied: rouge_score in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (0.1.2)\n","Requirement already satisfied: bert_score in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (0.3.13)\n","Requirement already satisfied: meteor in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (2.0.16)\n","Requirement already satisfied: datasets>=2.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (2.21.0)\n","Requirement already satisfied: numpy>=1.17 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (0.24.6)\n","Requirement already satisfied: packaging in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from evaluate) (23.2)\n","Requirement already satisfied: absl-py in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from rouge_score) (2.1.0)\n","Requirement already satisfied: nltk in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: torch>=1.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from bert_score) (2.4.0)\n","Requirement already satisfied: transformers>=3.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from bert_score) (4.44.2)\n","Requirement already satisfied: matplotlib in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from bert_score) (3.9.2)\n","Requirement already satisfied: bgzip<0.6.0,>=0.5.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (0.5.0)\n","Requirement already satisfied: biom-format<3.0.0,>=2.1.15 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (2.1.16)\n","Requirement already satisfied: cogent3<2025.0.0,>=2024.2.5a1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (2024.7.19a5)\n","Requirement already satisfied: ete3<4.0.0,>=3.1.3 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (3.1.3)\n","Requirement already satisfied: pyarrow<16.0.0,>=15.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (15.0.2)\n","Requirement already satisfied: pysam<0.23.0,>=0.22.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from meteor) (0.22.1)\n","Requirement already satisfied: click in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from biom-format<3.0.0,>=2.1.15->meteor) (8.1.7)\n","Requirement already satisfied: scipy>=1.3.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from biom-format<3.0.0,>=2.1.15->meteor) (1.14.1)\n","Requirement already satisfied: h5py in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from biom-format<3.0.0,>=2.1.15->meteor) (3.11.0)\n","Requirement already satisfied: chardet in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (5.2.0)\n","Requirement already satisfied: numba>=0.59.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (0.60.0)\n","Requirement already satisfied: scitrack in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (2021.5.3)\n","Requirement already satisfied: stevedore in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (5.3.0)\n","Requirement already satisfied: typing_extensions in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from cogent3<2025.0.0,>=2024.2.5a1->meteor) (4.12.2)\n","Requirement already satisfied: filelock in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n","Requirement already satisfied: aiohttp in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: sympy in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (1.13.2)\n","Requirement already satisfied: networkx in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.3)\n","Requirement already satisfied: jinja2 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: setuptools in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (72.2.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.82)\n","Requirement already satisfied: regex!=2019.12.17 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (2024.7.24)\n","Requirement already satisfied: safetensors>=0.4.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=8 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from matplotlib->bert_score) (3.1.4)\n","Requirement already satisfied: joblib in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from numba>=0.59.0->cogent3<2025.0.0,>=2024.2.5a1->meteor) (0.43.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n","Requirement already satisfied: pbr>=2.0.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from stevedore->cogent3<2025.0.0,>=2024.2.5a1->meteor) (6.1.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"]}],"source":["!pip install evaluate pyrouge rouge_score bert_score meteor"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:10:46.879754Z","iopub.status.busy":"2024-09-04T21:10:46.879360Z","iopub.status.idle":"2024-09-04T21:10:50.433980Z","shell.execute_reply":"2024-09-04T21:10:50.433090Z","shell.execute_reply.started":"2024-09-04T21:10:46.879716Z"},"id":"dHQv7Aeai2Nr","outputId":"6c4222bd-bf8f-4588-9871-716bd2cee0ca","scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/dawidk/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/dawidk/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /home/dawidk/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /home/dawidk/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import evaluate, nltk, csv\n","rouge = evaluate.load(\"rouge\")\n","meteor = evaluate.load(\"meteor\")\n","bertscore = evaluate.load(\"bertscore\")\n","\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T22:12:55.393574Z","iopub.status.busy":"2024-09-04T22:12:55.392930Z","iopub.status.idle":"2024-09-04T22:12:55.409241Z","shell.execute_reply":"2024-09-04T22:12:55.408258Z","shell.execute_reply.started":"2024-09-04T22:12:55.393517Z"},"trusted":true},"outputs":[],"source":["def compute_metrics_abs(eval_pred):\n","    predictions, labels = eval_pred\n","    \n","    with tokenizer.as_target_tokenizer():\n","        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","\n","    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    bert_scores = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n","    bert_scores.pop('hashcode')\n","    result = {\n","      **{f\"rouge/{k}\": round(v, 4) for k,v in rouge_scores.items()},\n","      **{f\"bertscore/bertscore-{k}\": round(np.mean(v), 4) for k,v in bert_scores.items()},\n","      'meteor': round(meteor.compute(predictions=decoded_preds, references=decoded_labels)['meteor'], 4),\n","    }\n","   \n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result\n"]},{"cell_type":"markdown","metadata":{"id":"4W_3h_eHi2Nu"},"source":["## Train and Evaluate"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:10:50.447836Z","iopub.status.busy":"2024-09-04T21:10:50.447465Z","iopub.status.idle":"2024-09-04T21:11:45.188795Z","shell.execute_reply":"2024-09-04T21:11:45.187621Z","shell.execute_reply.started":"2024-09-04T21:10:50.447795Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_bart)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:11:45.190413Z","iopub.status.busy":"2024-09-04T21:11:45.190066Z","iopub.status.idle":"2024-09-04T21:11:45.197177Z","shell.execute_reply":"2024-09-04T21:11:45.196276Z","shell.execute_reply.started":"2024-09-04T21:11:45.190341Z"},"id":"irJUvr6gi2No","trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:17:57.361542Z","iopub.status.busy":"2024-09-04T21:17:57.361148Z","iopub.status.idle":"2024-09-04T21:17:58.546418Z","shell.execute_reply":"2024-09-04T21:17:58.545351Z","shell.execute_reply.started":"2024-09-04T21:17:57.361502Z"},"trusted":true},"outputs":[],"source":["my_batch = data_collator(tokenized_tweetsumm_abs['train'])\n","assert len(my_batch) == 4 # default setting for the model"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["EXPERIMENT_PARAMS = []\n","BASE_PARAMS = {'lr':3e-5, 'batch_size':4, 'epochs': 6}\n","EXPERIMENT_PARAMS.append(BASE_PARAMS)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T22:02:56.144544Z","iopub.status.busy":"2024-09-04T22:02:56.144093Z","iopub.status.idle":"2024-09-04T22:02:56.149499Z","shell.execute_reply":"2024-09-04T22:02:56.148403Z","shell.execute_reply.started":"2024-09-04T22:02:56.144506Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["LEARN_RATES = (3e-5, 3e-4, 3e-6)\n","BATCH_SIZES = (4, 2, 8)\n","EPOCHS = (6,10)\n","\n","for lr in LEARN_RATES:\n","    for batch_size in BATCH_SIZES:\n","        for epoch in EPOCHS:\n","            if lr == BASE_PARAMS['lr'] and batch_size == BASE_PARAMS['batch_size'] and epoch == BASE_PARAMS['epochs']:\n","                continue\n","            experiment = {'lr':lr, 'batch_size':batch_size, }\n","            EXPERIMENT_PARAMS.append(experiment)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def run_post_training(split, test_details, test_df_temp: pd.DataFrame, tokenizer, experiment, run_name_model):\n","    preds = tokenizer.batch_decode(test_details.predictions, skip_special_tokens=True)\n","    test_df_temp['response'] = preds\n","    exp_res = None\n","    csv_items = {**experiment, **(test_details.metrics)}\n","    if not exp_res:\n","        exp_res = {k: list() for k in csv_items.keys()}\n","    else:\n","        for k, v in csv_items.items():\n","            exp_res[k].append(v)\n","\n","    test_metrics_df = pd.DataFrame(exp_res)\n","    print(test_metrics_df.head())\n","    test_df_temp.convert_dtypes()\n","    test_metrics_df.convert_dtypes()\n","    print(test_df_temp.dtypes)\n","    print(test_metrics_df.dtypes)\n","    print(test_df_temp.head())\n","    print(test_metrics_df.head())\n","    wandb.log({run_name_model: test_details.metrics})\n","    preds_name = f\"{split}_preds_{run_name_model.replace('-','_')}_bart.csv\"\n","    metrics_name =  f\"{split}_metrics_{run_name_model.replace('-','_')}_bart.csv\"\n","    test_df_temp.to_csv(os.path.join(os.getcwd(), 'results', preds_name), index=False, header=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n","    test_metrics_df.to_csv(os.path.join(os.getcwd(), 'results', metrics_name), index=False, header=True, encoding='utf-8', quoting=csv.QUOTE_ALL)\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def get_current_time():\n","    return datetime.datetime.now().strftime(\"%d%m-%H%M\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T22:14:49.321022Z","iopub.status.busy":"2024-09-04T22:14:49.320028Z","iopub.status.idle":"2024-09-04T22:28:53.635536Z","shell.execute_reply":"2024-09-04T22:28:53.633830Z","shell.execute_reply.started":"2024-09-04T22:14:49.320977Z"},"id":"nj5v3nT9i2Nw","outputId":"512d759b-0343-4f83-e36c-cf15fec503a3","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"]},{"name":"stdout","output_type":"stream","text":["Starting bart-abs-0509-2104-lr-3e-05-bs-4-ep-6 training\n"]},{"name":"stderr","output_type":"stream","text":["/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [109/109 02:43, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge/rouge1</th>\n","      <th>Rouge/rouge2</th>\n","      <th>Rouge/rougel</th>\n","      <th>Rouge/rougelsum</th>\n","      <th>Bertscore/bertscore-precision</th>\n","      <th>Bertscore/bertscore-recall</th>\n","      <th>Bertscore/bertscore-f1</th>\n","      <th>Meteor</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.232200</td>\n","      <td>2.133909</td>\n","      <td>0.461100</td>\n","      <td>0.216500</td>\n","      <td>0.392500</td>\n","      <td>0.393800</td>\n","      <td>0.904100</td>\n","      <td>0.889600</td>\n","      <td>0.896700</td>\n","      <td>0.397800</td>\n","      <td>32.527273</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'length_penalty': 0.5, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'length_penalty': 0.5, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"name":"stdout","output_type":"stream","text":["Finished. Time it took for training: 0:02:48.133706\n"]},{"name":"stderr","output_type":"stream","text":["/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/dawidk/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"ename":"OverflowError","evalue":"out of range integral type conversion attempted","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     43\u001b[0m     test_details \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(tokenized_tweetsumm_abs[split], metric_key_prefix\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mrun_post_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPD_DATASETS\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub(run_name_model)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mrun_post_training\u001b[0;34m(split, test_details, test_df_temp, tokenizer, experiment, run_name_model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_post_training\u001b[39m(split, test_details, test_df_temp: pd\u001b[38;5;241m.\u001b[39mDataFrame, tokenizer, experiment, run_name_model):\n\u001b[0;32m----> 2\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_details\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test_df_temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds\n\u001b[1;32m      4\u001b[0m     exp_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3953\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3954\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3957\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3958\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3961\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3977\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3983\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3984\u001b[0m     ]\n","File \u001b[0;32m~/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/dkbert/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:651\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    650\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 651\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    654\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    657\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n","\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"]}],"source":["exp_res = None\n","for exp in EXPERIMENT_PARAMS:\n","    current_time = get_current_time()\n","    run_name_model = f\"bart-abs-{current_time}-lr-{exp['lr']}-bs-{exp['batch_size']}-ep-{exp['epochs']}\"\n","    print(\"Starting\", run_name_model, \"training\")\n","    wandb.run.name = run_name_model\n","    wandb.run.save()\n","\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=os.path.join('..', f\"trained-distilbart-abs-{current_time}\"),\n","        eval_strategy=\"epoch\",\n","        logging_strategy=\"steps\",\n","        logging_steps=10,\n","        learning_rate=exp['lr'],\n","        per_device_train_batch_size=exp['batch_size'],\n","        per_device_eval_batch_size=exp['batch_size'],\n","        weight_decay=0.01,\n","        save_strategy=\"epoch\",\n","        save_total_limit=10,\n","        num_train_epochs=1,\n","        predict_with_generate=True,\n","        fp16=True,\n","        generation_max_length=80,\n","        push_to_hub=True,\n","        report_to=\"wandb\",\n","        run_name=run_name_model\n","    )\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_tweetsumm_abs[\"train\"],\n","        eval_dataset=tokenized_tweetsumm_abs[\"validation\"],\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics_abs,\n","    )\n","\n","    training_start = time.time()\n","    trainer.train()\n","    training_end = time.time()\n","    print(\"Finished. Time it took for training:\", str(datetime.timedelta(seconds=(training_end-training_start))))\n","    for split in ('train', 'validation', 'test'):\n","        test_details = trainer.predict(tokenized_tweetsumm_abs[split], metric_key_prefix=split)\n","        run_post_training(split, test_details, PD_DATASETS[split], tokenizer, exp, run_name_model)\n","    trainer.push_to_hub(run_name_model)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:37:23.937422Z","iopub.status.idle":"2024-09-03T17:37:23.937908Z","shell.execute_reply":"2024-09-03T17:37:23.937694Z","shell.execute_reply.started":"2024-09-03T17:37:23.937669Z"},"trusted":true},"outputs":[],"source":["print(\"Finished all training and evaluation\")\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["run_name_git = f\"bart-abs-{get_current_time()}\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%env BART_TRAINING_NAME={run_name_git}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!git add .\n","!sleep 2\n","!git commit -m \"Upload results $BART_TRAINING_NAME\"\n","!sleep 2\n","!git push origin main\n","!sleep 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Results uploaded\")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5430249,"sourceId":9040667,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09513166db494a318fe4bd2ff4fe8b73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bad09d132dcc420fbad7ace8cc25aa06","placeholder":"​","style":"IPY_MODEL_20787ae61e94452aae73869c5382ff1b","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"19649549a1ad47368ec9f6eeba1e80ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20787ae61e94452aae73869c5382ff1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25bd97f157d8485fb15438d25d83421c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c792771c4604d9b8f6bcda700f2ad74","placeholder":"​","style":"IPY_MODEL_36781c62f1164acabc7371ceb966e8cc","value":"Token is valid (permission: write)."}},"26b0af06bf1549bd8f16e297a3d5c4fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34efbeb9bdcf42b28bcfdf48289bb943":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36781c62f1164acabc7371ceb966e8cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bd1b8a5cbdf4345b34a6042ad3c6383":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d0bb7ba88104e458caaaf9c72914b97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44d30b0ecad54f818f27bbd1858cdf3f","placeholder":"​","style":"IPY_MODEL_85e73a9905ba41c286b0645f0c667362","value":"Login successful"}},"44d30b0ecad54f818f27bbd1858cdf3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5685a3fe6b084791b1c41bede0ee8429":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5764ccc3c9b943ac9031bb4fbf65ee84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dcb4bd1b7424f698dcfe1586ecaaa87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7393008614c94b7f9aee294715d1f9c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6eb7b2e427a4fb3b984a0346394555a","placeholder":"​","style":"IPY_MODEL_34efbeb9bdcf42b28bcfdf48289bb943","value":"Your token has been saved in your configured git credential helpers (store)."}},"7b5d107e75ec4226b2f951fa5eac32c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"85e73a9905ba41c286b0645f0c667362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c792771c4604d9b8f6bcda700f2ad74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91cf6c949361472e8cdc02a27371892b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_5764ccc3c9b943ac9031bb4fbf65ee84","placeholder":"​","style":"IPY_MODEL_f6f5badeb1f749d8a1a74c737ea45b81","value":""}},"9d286f232614404eac3e78415a823171":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26b0af06bf1549bd8f16e297a3d5c4fa","placeholder":"​","style":"IPY_MODEL_b289ce6929414af38bd4cd4ddf68bfe4","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a6333daeaf584192b2d8222134dc02a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb0ffca70a4645458b4650ff588f82cc","placeholder":"​","style":"IPY_MODEL_fec956e2f3ac4760a951569986ebb5c2","value":"Your token has been saved to /root/.cache/huggingface/token"}},"b289ce6929414af38bd4cd4ddf68bfe4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b292b711914d4f14abd2436aeb8050f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3bfd48d65b748349afd7e3e5000baff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"b6eb7b2e427a4fb3b984a0346394555a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bad09d132dcc420fbad7ace8cc25aa06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcab8a3e3c3f45d48890e88d50d8e982":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_b292b711914d4f14abd2436aeb8050f8","style":"IPY_MODEL_b3bfd48d65b748349afd7e3e5000baff","tooltip":""}},"c42b74a51b7843c7b8a9fca7d90bfd1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19649549a1ad47368ec9f6eeba1e80ff","placeholder":"​","style":"IPY_MODEL_3bd1b8a5cbdf4345b34a6042ad3c6383","value":"Connecting..."}},"d1a558c379ac48cc80c19ef9898d871d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_25bd97f157d8485fb15438d25d83421c","IPY_MODEL_7393008614c94b7f9aee294715d1f9c8","IPY_MODEL_a6333daeaf584192b2d8222134dc02a9","IPY_MODEL_3d0bb7ba88104e458caaaf9c72914b97"],"layout":"IPY_MODEL_7b5d107e75ec4226b2f951fa5eac32c7"}},"dbbfbb222a7d4ce18c1c7b655fc89ae1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_5685a3fe6b084791b1c41bede0ee8429","style":"IPY_MODEL_6dcb4bd1b7424f698dcfe1586ecaaa87","value":true}},"f6f5badeb1f749d8a1a74c737ea45b81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb0ffca70a4645458b4650ff588f82cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fec956e2f3ac4760a951569986ebb5c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
